from pyspark import SparkContext, SparkConf
import sys
import time
from random import randint
import numpy as np
import random
from pyspark.mllib.linalg import Vectors

def readVectorsSeq(filename):
    file = open(filename, 'r')
    vector_list = []
    for row in file.readlines():
        vector_list.append(Vectors.dense([float(num_str) for num_str in row.split()]))
    return vector_list

# K-medain distance function
def distance(a, b):
    return np.linalg.norm(a-b)
# K-medain distance between set of points and one center
def bulkDistance(center,points):
    return np.linalg.norm(points - center, axis=1)

# K-median distance from closest point
# p= point, s= set of points, i= valid number of points in s
def d(p,s,i=-1):
    if i == -1 : i=len(s)
    # a = np.array([distance(p,x) for x in s[0:i]])
    a = bulkDistance(p,s[0:i])
    return np.amin(a)

# probability function = wp*(d_p)/(sum_{q in p} w_q*(d_q))
# point = a single point, wpoint = weight of the point
# s= set of centers, wp= set of weights
def probability(point,p,s,wpoint, wp):
    sumation = 0
    for i,q in enumerate(p):
        sumation += d(q,s)*wp[i]
    return wpoint*d(point,s)/sumation

# draw random point from set p,
# with a probability function = w_p*(d_p)/(sum_{q non center} w_q*(d_q))
# s= set of centers, wp= set of weights
# returns index of the selected point
def randomPoint(p,s,wp):
    probSum = 0
    randValue = random.random() # random value between 0 and 1
    sumation = 0
    for i,q in enumerate(p):
        sumation += d(q,s)*wp[i]
        res=0
    for i, point in enumerate(p):
        probSum += wp[i]*d(point,s)/sumation
        if probSum > randValue: return i
    return -1 # error case

def partition(p,s,k):
    C = np.zeros(shape=(p.shape[0]+s.shape[0],p.shape[1]+1))
    pointNum = 0

    for ps in p:
        dist = np.array([distance(ps,x) for x in s])
        l = np.argmin(dist)

        #We add the index of the cluster at the end of the vector representing a point
        C[pointNum,:] = np.concatenate((ps,l),axis = None)
        pointNum += 1
    return C

def centroid(C,k):
    centroid = np.zeros(shape=(k,C.shape[1]-1))
    for i in range(k):
        pointNum = 0
        c_sum = np.zeros(C.shape[1]-1)
        for j in range(C.shape[0]):
            if C[j,-1] == i:
                c_sum += C[j,:-1]
                pointNum += 1
 
        centroid[i,:] = c_sum/pointNum
    return centroid

# K-means++
def kmeansPP(p,wp,k,iter):
    print("Running k-means++, to obtain initial centers...")
    pc = np.copy(p)
    wpc = np.copy(wp)
    s1 = random.randint(0,len(p)-1) # pick random starting point
    s= np.zeros(shape=(k,p.shape[1]))
    s[0] = np.copy(p[s1])
    pc = np.delete(pc,s1,0)
    wpc = np.delete(wpc,s1)
    for i in range(1,k):
        randPoint = randomPoint(pc,s,wpc)
        s[i] = np.copy(pc[randPoint])
        pc = np.delete(pc,randPoint,0)
        wpc = np.delete(wpc,randPoint)

    fi = float('inf')
    print("Running Lloydâ€™s algorithm...")
    stopping_condition = 0
    for i in range (0,iter):
        C = partition(p,s,k)
        cent = centroid(C,k) #Centroids of each cluster
        fi_kmeans = 0
        for cc in C:
            for i in range(k):
                if cc[-1] == i:
                    fi_kmeans += distance(cc[:-1],s[i,:])

        if fi_kmeans < fi:
            fi = fi_kmeans
            s = cent
        else : break

    return s

# kmeansObj receives in input a set of points P and a set of centers C,
# and returns the average distance of a point of P from C
def kmeansObj(p,c):
    return np.sum([d(point,c) for point in p])/len(p)

filepath = str(sys.argv[1])
k = int(sys.argv[2])
iter =int( sys.argv[3])
# Read dataset
print("Reading data file...")
# p = np.asarray(readVectorsSeq("dataset.data"))
start_time = time.time()
p =  np.loadtxt('dataset.data')
wp = np.ones(len(p))
s= kmeansPP(p,wp,k,iter)
print("Running kmeansObj...")
result = kmeansObj(p,s)
print("kmeansObj = ",result) # initial set of centers generated by kmeans++
elapsed_time = time.time() - start_time
print("elapsed_time:",elapsed_time)